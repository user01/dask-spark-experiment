docker build -t dask2.7example:v0.4 .

docker run --rm -it -p 8786:8786 -p 8787:8787 dask2.7example:v0.4 dask-scheduler
docker run --rm -it dask2.7example:v0.4 dask-worker $(ifconfig | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*' | grep -Eo '([0-9]*\.){3}[0-9]*' | grep -v '127.0.0.1'):8786 --nthreads 2 --memory-limit 0.25
docker run --rm -it -p 8888:8888 dask2.7example:v0.4 jupyter notebook --ip=0.0.0.0 --allow-root

docker build -t spark-trial:v001 -f Dockerfile.spark .
docker run --rm -it -p 8081:8081 spark-trial:v001 bin/spark-class org.apache.spark.deploy.worker.Worker spark://10.22.1.216:7077 -c 1 -m 1GB
docker run --rm -it spark-trial:v001 bin/spark-class org.apache.spark.deploy.worker.Worker spark://10.22.1.216:7077 -c 1 -m 1GB
docker run --rm -it spark-trial:v001 bin/spark-class org.apache.spark.deploy.worker.Worker spark://10.22.1.216:7077 -c 6 -m 12GB
docker run --rm -it -p 8080:8080 -p 7001:7001 -p 7002:7002 -p 7003:7003 -p 7004:7004 -p 7005:7005 -p 7006:7006 -p 7077:7077 -p 6066:6066 spark-trial:v001 bin/spark-class org.apache.spark.deploy.master.Master
docker run --rm -it -p 8080:8080 -p 7077:7077 spark-trial:v001 bin/spark-class org.apache.spark.deploy.master.Master
docker run --rm -it -p 8888:8080 spark-trial:v001 bash
